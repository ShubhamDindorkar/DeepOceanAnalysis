# Project Structure - Deep Ocean Analysis

## ğŸ“ Complete Directory Tree

```
DeepOceanAnalysis/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                    # Comprehensive project documentation
â”œâ”€â”€ ğŸ“„ SETUP.md                     # Installation and setup guide
â”œâ”€â”€ ğŸ“„ PROJECT_STRUCTURE.md         # This file
â”œâ”€â”€ ğŸ“„ requirements.txt             # Python dependencies
â”œâ”€â”€ ğŸ“„ config.yaml                  # Pipeline configuration
â”œâ”€â”€ ğŸ“„ .env                         # API keys (DO NOT COMMIT)
â”œâ”€â”€ ğŸ“„ .gitignore                   # Git ignore rules
â”œâ”€â”€ ğŸ“„ main.py                      # Main entry point
â”‚
â”œâ”€â”€ ğŸ“‚ data/                        # All data files (gitignored)
â”‚   â”œâ”€â”€ ğŸ“‚ raw/                     # Raw input sequences
â”‚   â”œâ”€â”€ ğŸ“‚ processed/               # QC'd and cleaned sequences
â”‚   â”œâ”€â”€ ğŸ“‚ embeddings/              # Generated embeddings (.npy)
â”‚   â”œâ”€â”€ ğŸ“‚ faiss_index/             # FAISS index files
â”‚   â”œâ”€â”€ ğŸ“‚ blast_db/                # Downloaded BLAST databases
â”‚   â”œâ”€â”€ ğŸ“‚ results/                 # Analysis results
â”‚   â””â”€â”€ ğŸ“‚ clusters/                # Cluster assignments and summaries
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/                     # Core pipeline scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ fetch_ncbi.py               # Fetch sequences from NCBI
â”‚   â”œâ”€â”€ embed_sequences.py          # Generate embeddings (Cerebras/FAISS)
â”‚   â”œâ”€â”€ build_faiss.py              # Build and query FAISS index
â”‚   â”œâ”€â”€ run_blast.py                # BLAST verification
â”‚   â”œâ”€â”€ cluster_unknowns.py         # UMAP + HDBSCAN clustering
â”‚   â”œâ”€â”€ analyze_pipeline.py         # Full end-to-end pipeline
â”‚   â””â”€â”€ report_generator.py         # Report generation with Exa
â”‚
â”œâ”€â”€ ğŸ“‚ notebooks/                   # Jupyter notebooks
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ 00_exploration.ipynb        # Data exploration
â”‚   â”œâ”€â”€ 01_embedding.ipynb          # Embedding generation
â”‚   â”œâ”€â”€ 02_similarity.ipynb         # Similarity search
â”‚   â”œâ”€â”€ 03_clustering.ipynb         # Clustering experiments
â”‚   â””â”€â”€ 04_reporting.ipynb          # Report generation
â”‚
â”œâ”€â”€ ğŸ“‚ tests/                       # Unit and integration tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_faiss.py               # FAISS index tests
â”‚   â”œâ”€â”€ test_blast.py               # BLAST wrapper tests
â”‚   â”œâ”€â”€ test_embeddings.py          # Embedding generation tests
â”‚   â””â”€â”€ test_pipeline.py            # Full pipeline tests
â”‚
â”œâ”€â”€ ğŸ“‚ logs/                        # Pipeline execution logs
â”‚   â””â”€â”€ README.md
â”‚
â””â”€â”€ ğŸ“‚ docs/                        # Documentation
    â”œâ”€â”€ architecture.md             # System architecture
    â”œâ”€â”€ pipeline_flow.md            # Pipeline flow diagram
    â””â”€â”€ usage_examples.md           # Usage examples and workflows
```

## ğŸ¯ Key Components

### Main Entry Point

**`main.py`**
- CLI interface for running the complete pipeline
- Usage: `python main.py --input data.fasta --output results/`

### Core Scripts

| Script | Purpose | Key Technologies |
|--------|---------|------------------|
| `fetch_ncbi.py` | Download sequences from NCBI | BioPython, Entrez |
| `embed_sequences.py` | Generate DNA embeddings | DNABERT-2, Cerebras, PyTorch |
| `build_faiss.py` | Index & search embeddings | FAISS |
| `run_blast.py` | Verify matches with alignment | BLAST+ |
| `cluster_unknowns.py` | Cluster unknown sequences | UMAP, HDBSCAN |
| `analyze_pipeline.py` | Orchestrate full pipeline | All above |
| `report_generator.py` | Generate reports | Exa, HTML |

### Configuration Files

**`config.yaml`**
- Model selection (DNABERT-2, Nucleotide Transformer, etc.)
- FAISS index type and parameters
- Similarity thresholds
- Clustering parameters (UMAP, HDBSCAN)
- BLAST settings
- Exa integration settings

**`.env`**
- `CEREBRAS_API_KEY` - For Cerebras Cloud inference
- `EXA_API_KEY` - For literature lookup
- `NCBI_API_KEY` - For faster NCBI downloads (optional)
- `HF_TOKEN` - For HuggingFace gated models (optional)

## ğŸš€ Quick Start

### 1. Setup

```bash
# Install dependencies
pip install -r requirements.txt

# Configure API keys
cp .env.example .env
# Edit .env with your keys

# Configure NCBI email (required)
# Edit config.yaml: ncbi.email = "your@email.com"
```

### 2. Run Pipeline

```bash
# Full analysis
python main.py --input data/raw/sequences.fasta --output data/results/

# With verbose logging
python main.py -i data/raw/sequences.fasta -o results/ --verbose
```

### 3. View Results

```bash
# Open HTML report
open data/results/analysis_report.html  # Mac
start data/results/analysis_report.html  # Windows

# Or explore in Jupyter
jupyter notebook notebooks/04_reporting.ipynb
```

## ğŸ“Š Data Flow

```
Input FASTA
    â†“
Preprocessing (QC, dedup)
    â†“
Embedding Generation (DNABERT-2 / Cerebras)
    â†“
FAISS Indexing & Search
    â†“
    â”œâ”€â”€ High Similarity â†’ Known Branch â†’ BLAST Verify â†’ Report
    â”‚
    â””â”€â”€ Low Similarity â†’ Unknown Branch â†’ UMAP + HDBSCAN â†’ Clusters
                                                              â†“
                                                         Exa Literature
                                                              â†“
                                                         Final Report
```

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/

# Run specific test
pytest tests/test_faiss.py -v

# Run with coverage
pytest --cov=scripts tests/
```

## ğŸ“š Documentation

| Document | Description |
|----------|-------------|
| `README.md` | Complete project overview with goals and architecture |
| `SETUP.md` | Detailed installation and configuration guide |
| `docs/architecture.md` | System design and component details |
| `docs/pipeline_flow.md` | Visual pipeline flow diagrams |
| `docs/usage_examples.md` | Example workflows and commands |

## ğŸ”§ Customization

### Change Embedding Model

Edit `config.yaml`:
```yaml
model:
  name: "InstaDeepAI/nucleotide-transformer-v2-250m-multi-species"
```

### Adjust Similarity Threshold

Edit `config.yaml`:
```yaml
similarity:
  known_threshold: 0.85  # Stricter threshold
```

### Enable Cerebras

Edit `config.yaml`:
```yaml
model:
  use_cerebras: true
```

And set API key in `.env`:
```
CEREBRAS_API_KEY=your_key_here
```

## ğŸ“ Important Notes

### Files to Never Commit

- `.env` (contains API keys)
- `data/` directories (large datasets)
- `logs/*.log` (log files)
- `__pycache__/` (Python cache)

These are already in `.gitignore`.

### Required Before Running

1. âœ… Install dependencies (`pip install -r requirements.txt`)
2. âœ… Configure `.env` with API keys
3. âœ… Set `ncbi.email` in `config.yaml`
4. âœ… Prepare input FASTA file

### Optional Enhancements

- Install BLAST+ for verification: `conda install -c bioconda blast`
- Use GPU: Install `faiss-gpu` and set `device: "cuda"` in config
- Enable Exa: Set `EXA_API_KEY` and `exa.enabled: true`

## ğŸ“ Learning Path

1. **Start here**: `README.md` - Understand the project goals
2. **Setup**: `SETUP.md` - Get everything installed
3. **Architecture**: `docs/architecture.md` - Learn system design
4. **Try it**: `docs/usage_examples.md` - Run example workflows
5. **Explore**: `notebooks/` - Interactive analysis
6. **Customize**: Edit `config.yaml` for your needs

## ğŸ†˜ Getting Help

- **Documentation**: Check `docs/` directory
- **Examples**: See `docs/usage_examples.md`
- **Logs**: Review `logs/pipeline.log` for errors
- **Tests**: Run `pytest tests/ -v` to verify setup
- **Issues**: Open a GitHub issue (if applicable)

## ğŸ“¦ Deliverables

After running the pipeline, you'll have:

1. **Similarity matches** (`data/results/similarity_matches.csv`)
2. **Cluster assignments** (`data/results/clusters/cluster_labels.csv`)
3. **Cluster summaries** (`data/results/clusters/cluster_summary.csv`)
4. **Visualizations** (`data/results/clusters/clusters_umap.png`)
5. **HTML Report** (`data/results/analysis_report.html`)

---

## ğŸŒŠ Ready to analyze deep-sea sequences!

Run `python main.py --help` to get started.
